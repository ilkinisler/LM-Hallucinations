# LLM Hallucination ðŸ§ 

A curated list of resources, papers, and tools related to **Large Language Models (LLM) and Hallucination**.

---

## Overview
1. Problem Statement
Language models (LMs) frequently produce outputs that are syntactically convincing but factually incorrect or ungrounded in reliable sources. This phenomenon, referred to as hallucination, presents significant challenges in ensuring the reliability, trustworthiness, and practical adoption of AI systems in critical areas such as healthcare, education, and policy-making. The focus is on tracking AI-generated outputs, evaluating their groundedness, and ensuring their faithfulness to reliable sources to distinguish between real data and synthetic outputs

2. Key Terms
Hallucination: Outputs generated by a language model that are factually inaccurate or ungrounded in training data or real-world knowledge.
Groundedness: The extent to which an AI's outputs can be traced back to reliable and credible sources.
Faithfulness: The alignment between generated content and the source data it is derived from.
Synthetic Language Data: Text created by AI models rather than written by humans.
Traceability: The ability to track the origin of an AI-generated text and verify its authenticity.

---

## Challenges
Defining and Quantifying Hallucination:
Lack of universally accepted definitions and metrics for hallucination leads to inconsistencies in evaluation and comparison of methods.
Detection and Attribution of Hallucination:
Differentiating between true and synthetic data, especially in large-scale corpora, is challenging.
Language models often blend accurate facts with hallucinations, making detection harder.
Evaluation Benchmarks:
Current benchmarks focus on synthetic tasks but fail to generalize to real-world use cases or specialized domains like healthcare or law.
Scalability and Real-Time Analysis:
Scaling hallucination detection methods to handle the output of state-of-the-art models in real time is computationally intensive.
Lack of Grounded Data:
Models trained on noisy or biased data inherit these flaws, exacerbating hallucination problems.
Ethical and Legal Concerns:
Misidentifying or misattributing content as hallucinated can have legal and reputational implications.

---

## Evaluation Metrics and Benchmarks
1. Metrics:
Precision and Recall: Measuring how accurately hallucinations are detected.
Faithfulness Scores: E.g., QAGS (Question-Answering Faithfulness Score), where outputs are evaluated against a reference document.
Factual Consistency: Evaluated via human annotations or automated tools like TruthfulQA or FactCC.
Groundedness Metrics: Assess the extent to which generated content is anchored in reliable sources.
2. Benchmarks:
TruthfulQA: Focused on evaluating truthfulness in language models.
FEVER (Fact Extraction and Verification): A dataset designed to evaluate fact-checking models.
REALM: Retrieval-Augmented Language Model benchmark that measures groundedness.
XSum Faithfulness Dataset: Targets hallucinations in summarization tasks.
SITUATEDQA: An open-retrieval QA dataset where systems must produce the correct answer to a question given the temporal or geographical context. 

---

## Evaluation-Benchmark
### 2023
- **TruthfulQA: Measuring How Models Mimic Human Falsehoods** - [Author(s)](https://arxiv.org/pdf/2109.07958) - A benchmark with a focus on identifying when models mimic human falsehoods and GPT-judge (LM QA).
- **Paper Title** - [Author(s)](https://example.com) - Short description.

---

## Detection
- [Tool Name](https://example.com) - Description.

---
