{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type HHEMv2Config to instantiate a model of type HHEMv2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0111, 0.6474, 0.1290, 0.8969, 0.1846, 0.0050, 0.0543])\n",
      "Source: The capital of France is Berlin.\n",
      "Response: The capital of France is Paris.\n",
      "SCORE: 0.011\n",
      "\n",
      "Source: I am in California\n",
      "Response: I am in United States.\n",
      "SCORE: 0.647\n",
      "\n",
      "Source: I am in United States\n",
      "Response: I am in California.\n",
      "SCORE: 0.129\n",
      "\n",
      "Source: A person on a horse jumps over a broken down airplane.\n",
      "Response: A person is outdoors, on a horse.\n",
      "SCORE: 0.897\n",
      "\n",
      "Source: A boy is jumping on skateboard in the middle of a red bridge.\n",
      "Response: The boy skates down the sidewalk on a red bridge\n",
      "SCORE: 0.185\n",
      "\n",
      "Source: A man with blond-hair, and a brown shirt drinking out of a public water fountain.\n",
      "Response: A blond man wearing a brown shirt is reading a book.\n",
      "SCORE: 0.005\n",
      "\n",
      "Source: Mark Wahlberg was a fan of Manny.\n",
      "Response: Manny was a fan of Mark Wahlberg.\n",
      "SCORE: 0.054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "pairs = [ # Test data, List[Tuple[str, str]]\n",
    "    (\"The capital of France is Berlin.\", \"The capital of France is Paris.\"), # factual but hallucinated\n",
    "    ('I am in California', 'I am in United States.'), # Consistent\n",
    "    ('I am in United States', 'I am in California.'), # Hallucinated\n",
    "    (\"A person on a horse jumps over a broken down airplane.\", \"A person is outdoors, on a horse.\"),\n",
    "    (\"A boy is jumping on skateboard in the middle of a red bridge.\", \"The boy skates down the sidewalk on a red bridge\"),\n",
    "    (\"A man with blond-hair, and a brown shirt drinking out of a public water fountain.\", \"A blond man wearing a brown shirt is reading a book.\"),\n",
    "    (\"Mark Wahlberg was a fan of Manny.\", \"Manny was a fan of Mark Wahlberg.\")\n",
    "]\n",
    "\n",
    "# Step 1: Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'vectara/hallucination_evaluation_model', trust_remote_code=True)\n",
    "\n",
    "# Step 2: Use the model to predict\n",
    "predictions = model.predict(pairs) # note the predict() method. Do not do model(pairs). \n",
    "print(predictions)\n",
    "# tensor([0.0111, 0.6474, 0.1290, 0.8969, 0.1846, 0.0050, 0.0543])\n",
    "\n",
    "# Iterate through results and print scores\n",
    "for idx in range(len(pairs)):\n",
    "    print(f\"Source: {pairs[idx][0]}\")\n",
    "    print(f\"Response: {pairs[idx][1]}\")\n",
    "    print(f\"SCORE: {np.round(predictions[idx].item(), 3)}\\n\")  # Convert tensor to Python float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Universal Studios is in Orlando, Florida.\n",
      "Response: Universal Studios is in Los Angeles.\n",
      "SCORE: 0.012\n",
      "\n",
      "Source: Employees get free park tickets every quarter.\n",
      "Response: Employees get unlimited free tickets.\n",
      "SCORE: 0.018\n",
      "\n",
      "Source: At Universal Studios, employees have access to a portal where they can view their perks. Perks include discounts on food, merchandise, and tickets for friends and family. The information is updated quarterly, and employees can check the latest benefits online.\n",
      "Response: Universal Studios employees get discounts, but these are only for food. Friends and family cannot get any benefits, and there is no online portal.\n",
      "SCORE: 0.034\n",
      "\n",
      "Source: To report a technical issue in the park, employees should use the internal support system. They need to log in and select the department responsible. If urgent, they can also call a support number.\n",
      "Response: Employees should just call the front desk when something breaks.\n",
      "SCORE: 0.007\n",
      "\n",
      "Source: Universal Studios has three major theme parks.\n",
      "Response: Universal Studios has five theme parks.\n",
      "SCORE: 0.016\n",
      "\n",
      "Source: The Wizarding World of Harry Potter is in Universal Studios Florida.\n",
      "Response: Harry Potter Land is in Disneyland.\n",
      "SCORE: 0.027\n",
      "\n",
      "Source: Universal Studios is in Orlando, Florida.\n",
      "Response: Universal Studios Florida is located in Orlando.\n",
      "SCORE: 0.901\n",
      "\n",
      "Source: Employees receive quarterly free park tickets.\n",
      "Response: Employees get free tickets every three months.\n",
      "SCORE: 0.011\n",
      "\n",
      "Source: The employee portal provides access to perks like discounts on food, merchandise, and park tickets.\n",
      "Response: Employees can check their perks, such as food discounts and merchandise offers, on the portal.\n",
      "SCORE: 0.928\n",
      "\n",
      "Source: To report technical issues, employees must log in to the internal support system and choose a department.\n",
      "Response: Employees should use the internal system to report technical issues by selecting the relevant department.\n",
      "SCORE: 0.915\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding both hallucination and non-hallucination cases and rerunning the experiments\n",
    "pairs = [\n",
    "    # Simple fact-checking (Hallucination)\n",
    "    (\"Universal Studios is in Orlando, Florida.\", \"Universal Studios is in Los Angeles.\"),  # Incorrect\n",
    "    (\"Employees get free park tickets every quarter.\", \"Employees get unlimited free tickets.\"),  # Incorrect\n",
    "\n",
    "    # Longer factual consistency check (Hallucination)\n",
    "    (\n",
    "        \"At Universal Studios, employees have access to a portal where they can view their perks. \"\n",
    "        \"Perks include discounts on food, merchandise, and tickets for friends and family. \"\n",
    "        \"The information is updated quarterly, and employees can check the latest benefits online.\",\n",
    "        \"Universal Studios employees get discounts, but these are only for food. \"\n",
    "        \"Friends and family cannot get any benefits, and there is no online portal.\"  # Incorrect\n",
    "    ),\n",
    "    (\n",
    "        \"To report a technical issue in the park, employees should use the internal support system. \"\n",
    "        \"They need to log in and select the department responsible. If urgent, they can also call a support number.\",\n",
    "        \"Employees should just call the front desk when something breaks.\"  # Incorrect\n",
    "    ),\n",
    "\n",
    "    # Contradictions and misleading statements (Hallucination)\n",
    "    (\"Universal Studios has three major theme parks.\", \"Universal Studios has five theme parks.\"),  # Incorrect\n",
    "    (\"The Wizarding World of Harry Potter is in Universal Studios Florida.\", \"Harry Potter Land is in Disneyland.\"),  # Incorrect\n",
    "\n",
    "    # Non-hallucination (Correct paraphrases or factual matches)\n",
    "    (\"Universal Studios is in Orlando, Florida.\", \"Universal Studios Florida is located in Orlando.\"),  # Correct\n",
    "    (\"Employees receive quarterly free park tickets.\", \"Employees get free tickets every three months.\"),  # Correct\n",
    "    (\n",
    "        \"The employee portal provides access to perks like discounts on food, merchandise, and park tickets.\",\n",
    "        \"Employees can check their perks, such as food discounts and merchandise offers, on the portal.\"  # Correct\n",
    "    ),\n",
    "    (\n",
    "        \"To report technical issues, employees must log in to the internal support system and choose a department.\",\n",
    "        \"Employees should use the internal system to report technical issues by selecting the relevant department.\"  # Correct\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Step 2: Use the model to predict\n",
    "predictions = model.predict(pairs) # note the predict() method. Do not do model(pairs). \n",
    "\n",
    "# Iterate through results and print scores\n",
    "for idx in range(len(pairs)):\n",
    "    print(f\"Source: {pairs[idx][0]}\")\n",
    "    print(f\"Response: {pairs[idx][1]}\")\n",
    "    print(f\"SCORE: {np.round(predictions[idx].item(), 3)}\\n\")  # Convert tensor to Python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type HHEMv2Config to instantiate a model of type HHEMv2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for GPS-hallucination-dataset.csv...\n",
      "Number of samples: 26\n",
      "Precision: 0.917\n",
      "Recall: 0.846\n",
      "Execution Time per sample: 60.89 ms\n",
      "--------------------------------------\n",
      "Evaluating for OHS-hallucination-dataset.csv...\n",
      "Number of samples: 36\n",
      "Precision: 1.000\n",
      "Recall: 0.944\n",
      "Execution Time per sample: 46.22 ms\n",
      "--------------------------------------\n",
      "Evaluating for SubsystemController-hallucination-dataset.csv...\n",
      "Number of samples: 40\n",
      "Precision: 1.000\n",
      "Recall: 0.900\n",
      "Execution Time per sample: 36.09 ms\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "def load_data(csv_directory, dataset):\n",
    "    # List to store all (context, true sentence) pairs\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    # Read each CSV file and process it\n",
    "    for filename in os.listdir(csv_directory):\n",
    "        if filename.endswith(dataset):\n",
    "            file_path = os.path.join(csv_directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Ensure the required columns exist\n",
    "            if {\"Source/Context\", \"Ungrounded Response\", \"Grounded Response\"}.issubset(df.columns):\n",
    "                for _, row in df.iterrows():\n",
    "                    context = row[\"Source/Context\"]  # Source/Context\n",
    "                    grounded_sentence = row[\"Grounded Response\"]  # True factual statement\n",
    "                    ungrounded_sentence = row[\"Ungrounded Response\"]  # True factual statement\n",
    "\n",
    "                    # Append as a tuple to pairs\n",
    "                    pairs.append((context, grounded_sentence))\n",
    "                    labels.append(1)\n",
    "                    pairs.append((context, ungrounded_sentence))\n",
    "                    labels.append(0)\n",
    "    return pairs, labels\n",
    "\n",
    "def evaluate(model, csv_directory, dataset, debug=False):\n",
    "\n",
    "    # List to store all (context, true sentence) pairs\n",
    "    pairs, labels = load_data(csv_directory, dataset)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # Step 2: Use the model to predict\n",
    "    predictions = model.predict(pairs) # note the predict() method. Do not do model(pairs). \n",
    "    # tensor([0.0111, 0.6474, 0.1290, 0.8969, 0.1846, 0.0050, 0.0543])\n",
    "    end_time = time.time()\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    predicted_labels = (predictions > 0.5).int().numpy()\n",
    "\n",
    "    if debug:\n",
    "        # Iterate through results and print scores\n",
    "        for idx in range(len(pairs)):\n",
    "            print(f\"Source: {pairs[idx][0]}\")\n",
    "            print(f\"Response: {pairs[idx][1]}\")\n",
    "            print(f\"SCORE: {np.round(predictions[idx].item(), 3)}\\n\")  # Convert tensor to Python float\n",
    "\n",
    "    # Calculate true positives, false positives, false negatives\n",
    "    true_positives = np.sum((predicted_labels == 1) & (labels == 1))\n",
    "    false_positives = np.sum((predicted_labels == 1) & (labels == 0))\n",
    "    false_negatives = np.sum((predicted_labels == 0) & (labels == 1))\n",
    "\n",
    "    # Compute precision and recall\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    \n",
    "    # Calculate execution time in milliseconds\n",
    "    execution_time_ms = (end_time - start_time) * 1000\n",
    "    \n",
    "    print(f\"Evaluating for {dataset}...\")\n",
    "    print(f\"Number of samples: {len(pairs)}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"Execution Time per sample: {execution_time_ms/len(pairs):.2f} ms\")\n",
    "    print(f\"--------------------------------------\")\n",
    "\n",
    "    return\n",
    "\n",
    "# Step 1: Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'vectara/hallucination_evaluation_model', trust_remote_code=True)\n",
    "    \n",
    "csv_directory = \"/home/ilkin/Documents/UC/LM-Hallucinations-main/data\"\n",
    "evaluate(model, csv_directory, \"GPS-hallucination-dataset.csv\")\n",
    "evaluate(model, csv_directory, \"OHS-hallucination-dataset.csv\")\n",
    "evaluate(model, csv_directory, \"SubsystemController-hallucination-dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: General Performance Specification (GPS) v23.0 – Section 1.1.4 Operational Life and Availability: \"Equipment shall be designed to meet the required Availability during scheduled Operating Hours throughout the Operational Life when operated and maintained per the vendor-provided Operation and Maintenance Manuals. Operating hours are defined as 16 hours per day, which is equivalent to 5840 hours per year.\"\n",
      "Response: The required availability for system-level ride and show equipment is 99.5%, and the operational life is 20 years.\n",
      "SCORE: 0.01\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.1.4 Operational Life and Availability: \"Equipment shall be designed to meet the required Availability during scheduled Operating Hours throughout the Operational Life when operated and maintained per the vendor-provided Operation and Maintenance Manuals. Operating hours are defined as 16 hours per day, which is equivalent to 5840 hours per year.\"\n",
      "Response: The required availability for system-level ride and show equipment is 98.0%, and the operational life is 25 years.\n",
      "SCORE: 0.012\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.2.1 Fail-Safe Design: \"Type-1 equipment shall be fail-safe, meaning that if an SPF or MPF occurs in the Equipment or its component(s), no direct or indirect harm or consequential damage will occur to persons, the Equipment, or other equipment as a result of such failure.\"\n",
      "Response: Type-1 equipment must be fail-safe, ensuring that any single-point failure (SPF) or multiple-point failure (MPF) does not cause harm to people, the equipment, or adjacent equipment.\n",
      "SCORE: 0.796\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.2.1 Fail-Safe Design: \"Type-1 equipment shall be fail-safe, meaning that if an SPF or MPF occurs in the Equipment or its component(s), no direct or indirect harm or consequential damage will occur to persons, the Equipment, or other equipment as a result of such failure.\"\n",
      "Response: Type-1 equipment is fail-safe only when operated under controlled maintenance conditions and does not need to mitigate multiple-point failures (MPFs). \n",
      "SCORE: 0.017\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.2.3 Safety Requirements: \"Equipment shall be provided with adequate safety interlocks, protection and detection systems, and warning labels to prevent accidents involving any person or equipment.\" \n",
      "Response: Equipment must have safety interlocks, protection and detection systems, and warning labels to prevent accidents\n",
      "SCORE: 0.971\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.2.3 Safety Requirements: \"Equipment shall be provided with adequate safety interlocks, protection and detection systems, and warning labels to prevent accidents involving any person or equipment.\" \n",
      "Response: Equipment must have only physical barriers and warning labels to prevent accidents.\n",
      "SCORE: 0.036\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.1.6 Acceptable Equipment and Materials: \"Only new (not used or reconditioned) equipment, materials, and components shall be used. Use only off-the-shelf equipment and materials with industry-recognized standard dimensions and tolerances readily commercially available at the final installed location.\"\n",
      "Response: No, vendors must only use new equipment, materials, and components. Used or reconditioned equipment is not allowed. \n",
      "SCORE: 0.273\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.1.6 Acceptable Equipment and Materials: \"Only new (not used or reconditioned) equipment, materials, and components shall be used. Use only off-the-shelf equipment and materials with industry-recognized standard dimensions and tolerances readily commercially available at the final installed location.\"\n",
      "Response: Vendors are allowed to use reconditioned equipment if it meets industry standards. \n",
      "SCORE: 0.59\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.3.2 Stress Analysis: \"All Equipment shall be designed with allowable loads and stresses derived from the values obtained from a recognized publication (for example, Eurocode or AISC) and shall match the material composition, temperature, surface condition, stress application, local geometric factors, size, and heat treatment of the component being analyzed as closely as possible.\"\n",
      "Response: Stress analysis must be based on recognized publications like Eurocode or AISC, considering material properties, temperature, and other relevant factors.\n",
      "SCORE: 0.913\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.3.2 Stress Analysis: \"All Equipment shall be designed with allowable loads and stresses derived from the values obtained from a recognized publication (for example, Eurocode or AISC) and shall match the material composition, temperature, surface condition, stress application, local geometric factors, size, and heat treatment of the component being analyzed as closely as possible.\"\n",
      "Response: Stress analysis must be based solely on internal testing without reference to external standards.\n",
      "SCORE: 0.036\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.2.2 Safe State: \"A Safe State is achieved, in general, when all motion is halted in a controlled manner and all energy sources are disconnected from potentially dangerous components in a fail-safe way.\"\n",
      "Response: A Safe State is when all motion is halted in a controlled manner and all energy sources are disconnected from potentially dangerous components.\n",
      "SCORE: 0.927\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.2.2 Safe State: \"A Safe State is achieved, in general, when all motion is halted in a controlled manner and all energy sources are disconnected from potentially dangerous components in a fail-safe way.\"\n",
      "Response: A Safe State is when the equipment shuts down completely, including all power and diagnostic systems.\n",
      "SCORE: 0.012\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.1.5.1 General Requirements: \"Embedded diagnostics and predictive maintenance techniques are required for all major mechanical and electrical components, subsystems, and equipment with Mean Time Between Failures (MTBF) shorter than the life of the attraction.\"\n",
      "Response: Embedded diagnostics are required for all major mechanical and electrical components with MTBF shorter than the life of the attraction.\n",
      "SCORE: 0.952\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.1.5.1 General Requirements: \"Embedded diagnostics and predictive maintenance techniques are required for all major mechanical and electrical components, subsystems, and equipment with Mean Time Between Failures (MTBF) shorter than the life of the attraction.\"\n",
      "Response: Embedded diagnostics are only required for electrical components with a high failure rate.\n",
      "SCORE: 0.023\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.3.2.2 Load Cases: \"Loads generated by guests or any other persons must be considered in design calculations.\"\n",
      "Response: Yes, guest-induced loads must be considered in the equipment design calculations.\n",
      "SCORE: 0.578\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.3.2.2 Load Cases: \"Loads generated by guests or any other persons must be considered in design calculations.\"\n",
      "Response: No, guest-induced loads are not significant enough to be included in design calculations.\n",
      "SCORE: 0.042\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.1.3 Code Compliance: \"All ride systems shall comply with the requirements of the Americans with Disabilities Act (ADA).\"\n",
      "Response: Ride systems must comply with the Americans with Disabilities Act (ADA).\n",
      "SCORE: 0.965\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.1.3 Code Compliance: \"All ride systems shall comply with the requirements of the Americans with Disabilities Act (ADA).\"\n",
      "Response: Ride systems must comply with local accessibility guidelines, but ADA compliance is optional.\n",
      "SCORE: 0.009\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.1.5.3 Safety and Accessibility: \"Equipment shall be provided with a modular means to safely remove, discharge, or isolate all sources of active or stored energy (for example, kinetic, electrical, hydraulic, pneumatic) from portions of and across the entire system during maintenance activities.\"\n",
      "Response: Equipment must allow for safe removal, discharge, or isolation of all active or stored energy sources during maintenance.\n",
      "SCORE: 0.982\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.1.5.3 Safety and Accessibility: \"Equipment shall be provided with a modular means to safely remove, discharge, or isolate all sources of active or stored energy (for example, kinetic, electrical, hydraulic, pneumatic) from portions of and across the entire system during maintenance activities.\"\n",
      "Response: Only electrical energy needs to be isolated during maintenance, as other forms do not pose risks.\n",
      "SCORE: 0.048\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.3.1.1 High-Reliability Systems: \"All components must have published MTBF data greater than 25,000 operating hours or 250,000 cycles (whichever is applicable).\"\n",
      "Response: The minimum MTBF requirement for high-reliability system components is 25,000 operating hours or 250,000 cycles.\n",
      "SCORE: 0.753\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.3.1.1 High-Reliability Systems: \"All components must have published MTBF data greater than 25,000 operating hours or 250,000 cycles (whichever is applicable).\"\n",
      "Response: The minimum MTBF requirement for high-reliability system components is 10,000 operating hours or 100,000 cycles.\n",
      "SCORE: 0.117\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.3.3 Bearing Systems: \"Bearings shall be selected to exceed the manufacturer’s L10 life criteria for the application and shall be selected two sizes over the L10 life criteria.\"\n",
      "Response: Bearings must exceed the manufacturer’s L10 life criteria and be selected two sizes over the L10 life criteria.\n",
      "SCORE: 0.975\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.3.3 Bearing Systems: \"Bearings shall be selected to exceed the manufacturer’s L10 life criteria for the application and shall be selected two sizes over the L10 life criteria.\"\n",
      "Response: Bearings must meet the manufacturer’s L10 life criteria exactly, with no additional size requirements.\n",
      "SCORE: 0.15\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.2.4.2 Failure Analysis Report (FAR): \"A detailed Failure Analysis Report (FAR), such as an FMEA Report or Fault Tree Analysis, is required for all Equipment Elements.\"\n",
      "Response: A Failure Analysis Report (FAR), such as an FMEA Report or Fault Tree Analysis, is required for all equipment elements.\n",
      "SCORE: 0.935\n",
      "\n",
      "Source: General Performance Specification (GPS) v23.0 – Section 1.2.4.2 Failure Analysis Report (FAR): \"A detailed Failure Analysis Report (FAR), such as an FMEA Report or Fault Tree Analysis, is required for all Equipment Elements.\"\n",
      "Response: A Failure Analysis Report (FAR) is only required for Type-1 equipment.\n",
      "SCORE: 0.005\n",
      "\n",
      "Evaluating for GPS-hallucination-dataset.csv...\n",
      "Number of samples: 26\n",
      "Precision: 0.917\n",
      "Recall: 0.846\n",
      "Execution Time per sample: 56.10 ms\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, csv_directory, \"GPS-hallucination-dataset.csv\", debug = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type HHEMv2Config to instantiate a model of type HHEMv2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of document: 225344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (754 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for GPS-hallucination-dataset.csv...\n",
      "Number of samples: 26\n",
      "Precision: 1.000\n",
      "Recall: 0.692\n",
      "Execution Time per sample: 10.18 ms\n",
      "--------------------------------------\n",
      "Length of document: 39686\n",
      "Evaluating for OHS-hallucination-dataset.csv...\n",
      "Number of samples: 36\n",
      "Precision: 0.700\n",
      "Recall: 0.389\n",
      "Execution Time per sample: 8.52 ms\n",
      "--------------------------------------\n",
      "Length of document: 118539\n",
      "Evaluating for SubsystemController-hallucination-dataset.csv...\n",
      "Number of samples: 40\n",
      "Precision: 1.000\n",
      "Recall: 0.050\n",
      "Execution Time per sample: 6.59 ms\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# 1. Preprocess your document: Assume you already have your document as a string\n",
    "def split_text_into_chunks(text, max_chunk_size=400, overlap=50):\n",
    "    \"\"\"\n",
    "    Splits a long text into chunks of up to `max_chunk_size` words,\n",
    "    with an overlap of `overlap` words between consecutive chunks.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(start + max_chunk_size, len(words))\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += max_chunk_size - overlap  # move forward with some overlap\n",
    "    return chunks\n",
    "\n",
    "def get_related_chunk(hypothesis, chunks):\n",
    "\n",
    "    # 2. Initialize the embedding model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  # A lightweight model for generating embeddings\n",
    "\n",
    "    # Encode all document chunks (you might want to cache these embeddings for efficiency)\n",
    "    chunk_embeddings = model.encode(chunks, convert_to_tensor=True)\n",
    "\n",
    "    # 3. Define your hypothesis (the sentence you want to check) and compute its embedding\n",
    "    query_embedding = model.encode(hypothesis, convert_to_tensor=True)\n",
    "\n",
    "    # 4. Compute cosine similarity between the hypothesis and each chunk\n",
    "    cosine_scores = util.cos_sim(query_embedding, chunk_embeddings)[0]\n",
    "\n",
    "    # 5. Retrieve the top-k most similar chunks (e.g., top 3)\n",
    "    top_k = 1\n",
    "    top_results = torch.topk(cosine_scores, k=top_k)\n",
    "\n",
    "    top_picks = []\n",
    "    #print(\"Top related document chunks:\")\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "    #    print(idx)\n",
    "    #    print(f\"\\nChunk (Score: {score.item():.4f}):\\n{chunks[idx]}\")\n",
    "    #    print(\"Length of document:\", len(chunks[idx]))\n",
    "        top_picks.append(chunks[idx])\n",
    "\n",
    "    return top_picks\n",
    "\n",
    "def load_data(csv_directory, dataset, chunks):\n",
    "    # List to store all (context, true sentence) pairs\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    # Read each CSV file and process it\n",
    "    for filename in os.listdir(csv_directory):\n",
    "        if filename.endswith(dataset):\n",
    "            file_path = os.path.join(csv_directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Ensure the required columns exist\n",
    "            if {\"Source/Context\", \"Ungrounded Response\", \"Grounded Response\"}.issubset(df.columns):\n",
    "                for _, row in df.iterrows():\n",
    "                    grounded_sentence = row[\"Grounded Response\"]  # True factual statement\n",
    "                    context_3 = get_related_chunk(grounded_sentence , chunks)  # Source/Context\n",
    "                    # Append as a tuple to pairs\n",
    "                    pairs.append((context_3, grounded_sentence))\n",
    "                    labels.append(1)\n",
    "\n",
    "                    ungrounded_sentence = row[\"Ungrounded Response\"]  # True factual statement\n",
    "                    context_3 = get_related_chunk(ungrounded_sentence, chunks)  # Source/Context\n",
    "                    # Append as a tuple to pairs\n",
    "                    pairs.append((context_3, ungrounded_sentence))\n",
    "                    labels.append(0)\n",
    "    return pairs, labels\n",
    "\n",
    "def evaluate(model, csv_directory, dataset, chunks, debug=False):\n",
    "\n",
    "    # List to store all (context, true sentence) pairs\n",
    "    pairs, labels = load_data(csv_directory, dataset, chunks)\n",
    "    predictions = []\n",
    "    for each_context, hypotesis in pairs:\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Step 2: Use the model to predict\n",
    "        prediction1 = model.predict([(each_context[0], hypotesis)]) # note the predict() method. Do not do model(pairs). \n",
    "        #prediction2 = model.predict([(each_context[1], hypotesis)]) # note the predict() method. Do not do model(pairs). \n",
    "        end_time = time.time()\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        # Ensure all predictions are tensors\n",
    "        #predictions_tensor = torch.stack([prediction1, prediction2])  # Stack predictions\n",
    "        predictions_tensor = torch.stack([prediction1])  # Stack predictions\n",
    "\n",
    "        # Get max value across predictions\n",
    "        max_prediction = torch.max(predictions_tensor, dim=0)[0]  # Take max along dimension 0\n",
    "\n",
    "        # Append max_prediction to predictions list\n",
    "        predictions.append(max_prediction)\n",
    "    predictions_tensor = torch.tensor(predictions)  # Convert list to tensor\n",
    "    predicted_labels = (predictions_tensor > 0.5).int().numpy()\n",
    "\n",
    "    if debug:\n",
    "        # Iterate through results and print scores\n",
    "        for idx in range(len(pairs)):\n",
    "            print(f\"Source: {pairs[idx][0]}\")\n",
    "            print(f\"Response: {pairs[idx][1]}\")\n",
    "            print(f\"SCORE: {np.round(predictions[idx].item(), 3)}\\n\")  # Convert tensor to Python float\n",
    "    # Calculate true positives, false positives, false negatives\n",
    "    true_positives = np.sum((predicted_labels == 1) & (labels == 1))\n",
    "    false_positives = np.sum((predicted_labels == 1) & (labels == 0))\n",
    "    false_negatives = np.sum((predicted_labels == 0) & (labels == 1))\n",
    "\n",
    "    # Compute precision and recall\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    \n",
    "    # Calculate execution time in milliseconds\n",
    "    execution_time_ms = (end_time - start_time) * 1000\n",
    "    \n",
    "    print(f\"Evaluating for {dataset}...\")\n",
    "    print(f\"Number of samples: {len(pairs)}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"Execution Time per sample: {execution_time_ms/len(pairs):.2f} ms\")\n",
    "    print(f\"--------------------------------------\")\n",
    "\n",
    "    return\n",
    "\n",
    "csv_directory = \"/home/ilkin/Documents/UC/LM-Hallucinations-main/data\"\n",
    "# Step 1: Load the model\n",
    "vectara_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'vectara/hallucination_evaluation_model', trust_remote_code=True)\n",
    "\n",
    "with open(\"/home/ilkin/Documents/UC/LM-Hallucinations-main/data/UC-documents/GPS 23.0 1.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "    gps_string = file.read()\n",
    "print(\"Length of document:\", len(gps_string))\n",
    "# Split the document into chunks\n",
    "chunks = split_text_into_chunks(gps_string, max_chunk_size=400, overlap=50)\n",
    "evaluate(vectara_model, csv_directory, \"GPS-hallucination-dataset.csv\", chunks)\n",
    "\n",
    "with open(\"/home/ilkin/Documents/UC/LM-Hallucinations-main/data/UC-documents/OHS 3.2.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "    ohs_string = file.read()\n",
    "print(\"Length of document:\", len(ohs_string))\n",
    "# Split the document into chunks\n",
    "chunks = split_text_into_chunks(gps_string, max_chunk_size=400, overlap=50)\n",
    "evaluate(vectara_model, csv_directory, \"OHS-hallucination-dataset.csv\",  chunks)\n",
    "\n",
    "with open(\"/home/ilkin/Documents/UC/LM-Hallucinations-main/data/UC-documents/Subsystem Controller Specification (SSC).md\", \"r\", encoding=\"utf-8\") as file:\n",
    "    subsystem_string = file.read()\n",
    "print(\"Length of document:\", len(subsystem_string))\n",
    "# Split the document into chunks\n",
    "chunks = split_text_into_chunks(gps_string, max_chunk_size=400, overlap=50)\n",
    "evaluate(vectara_model, csv_directory, \"SubsystemController-hallucination-dataset.csv\",  chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "400, 50, 3\n",
    "Precision: 1.000\n",
    "Recall: 0.692\n",
    "Execution Time per sample: 31.36 ms\n",
    "\n",
    "400, 50, 2\n",
    "Precision: 1.000\n",
    "Recall: 0.692\n",
    "Execution Time per sample: 20.44 ms\n",
    "\n",
    "300, 50, 2\n",
    "Precision: 0.857\n",
    "Recall: 0.462\n",
    "Execution Time per sample: 14.27 ms\n",
    "\n",
    "450, 50, 2\n",
    "Precision: 1.000\n",
    "Recall: 0.462\n",
    "Execution Time per sample: 25.79 ms\n",
    "\n",
    "350, 50, 2\n",
    "Precision: 1.000\n",
    "Recall: 0.462\n",
    "Execution Time per sample: 17.57 ms\n",
    "\n",
    "400, 50, 1\n",
    "Precision: 1.000\n",
    "Recall: 0.692\n",
    "Execution Time per sample: 10.16 ms\n",
    "400, 50, 3\n",
    "Precision: 1.000\n",
    "Recall: 0.692\n",
    "Execution Time per sample: 31.36 ms\n",
    "\n",
    "400, 50, 2\n",
    "Precision: 1.000\n",
    "Recall: 0.692\n",
    "Execution Time per sample: 20.44 ms\n",
    "\n",
    "300, 50, 2\n",
    "Precision: 0.857\n",
    "Recall: 0.462\n",
    "Execution Time per sample: 14.27 ms\n",
    "\n",
    "450, 50, 2\n",
    "Precision: 1.000\n",
    "Recall: 0.462\n",
    "Execution Time per sample: 25.79 ms\n",
    "\n",
    "350, 50, 2\n",
    "Precision: 1.000\n",
    "Recall: 0.462\n",
    "Execution Time per sample: 17.57 ms\n",
    "\n",
    "400, 50, 1\n",
    "Precision: 1.000\n",
    "Recall: 0.692\n",
    "Execution Time per sample: 10.16 ms\n",
    "\n",
    "350, 50, 1\n",
    "Precision: 1.000\n",
    "Recall: 0.308\n",
    "Execution Time per sample: 8.34 ms\n",
    "\n",
    "450, 50, 1\n",
    "Precision: 1.000\n",
    "Recall: 0.308\n",
    "Execution Time per sample: 11.82 ms\n",
    "\n",
    "350, 50, 1\n",
    "Precision: 1.000\n",
    "Recall: 0.308\n",
    "Execution Time per sample: 8.34 ms\n",
    "\n",
    "450, 50, 1\n",
    "Precision: 1.000\n",
    "Recall: 0.308\n",
    "Execution Time per sample: 11.82 ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average string length in column 'Source/Context': 287.85\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"/home/ilkin/Documents/UC/LM-Hallucinations-main/data/GPS-hallucination-dataset.csv\")\n",
    "\n",
    "# Specify the column name\n",
    "column_name = \"Source/Context\"\n",
    "\n",
    "# Compute the average string length\n",
    "average_length = df[column_name].astype(str).apply(len).mean()\n",
    "\n",
    "print(f\"Average string length in column '{column_name}': {average_length:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of document: 225344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type HHEMv2Config to instantiate a model of type HHEMv2. This is not supported for all configurations of models and can yield errors.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (51202 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:118] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 503355802368 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectara/hallucination_evaluation_model\u001b[39m\u001b[38;5;124m'\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Step 2: Use the model to predict\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# note the predict() method. Do not do model(pairs). \u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# tensor([0.0111, 0.6474, 0.1290, 0.8969, 0.1846, 0.0050, 0.0543])\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Iterate through results and print scores\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/vectara/hallucination_evaluation_model/b3973afb9f9595a40bb8403b46c6dac9c26d16d5/modeling_hhem_v2.py:65\u001b[0m, in \u001b[0;36mHHEMv2ForSequenceClassification.predict\u001b[0;34m(self, text_pairs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt5\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 65\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt5\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits    \n\u001b[1;32m     67\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m0\u001b[39m, :] \u001b[38;5;66;03m# tok_cls\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:2273\u001b[0m, in \u001b[0;36mT5ForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   2268\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   2269\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2271\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 2273\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2274\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2277\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2278\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2279\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2283\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2284\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:2086\u001b[0m, in \u001b[0;36mT5EncoderModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;124;03m>>> last_hidden_states = outputs.last_hidden_state\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m   2084\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 2086\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2094\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoder_outputs\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1124\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1108\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1109\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         cache_position,\n\u001b[1;32m   1122\u001b[0m     )\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:675\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    661\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    673\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m ):\n\u001b[0;32m--> 675\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m     hidden_states, past_key_value \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    686\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:593\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    583\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    591\u001b[0m ):\n\u001b[1;32m    592\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 593\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    604\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/UC/.conda/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:520\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    517\u001b[0m             past_key_value\u001b[38;5;241m.\u001b[39mis_updated[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    523\u001b[0m     key_length \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:118] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 503355802368 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "with open(\"/home/ilkin/Documents/UC/LM-Hallucinations-main/data/UC-documents/GPS 23.0 1.md\", \"r\", encoding=\"utf-8\") as file:\n",
    "    gps_string = file.read()\n",
    "print(\"Length of document:\", len(gps_string))\n",
    "\n",
    "pairs = [ # Test data, List[Tuple[str, str]]\n",
    "    (gps_string, \"The required availability for system-level ride and show equipment is 99.5%, and the operational life is 20 years.\"), # factual but hallucinated\n",
    "    (gps_string, 'The required availability for system-level ride and show equipment is 98.0%, and the operational life is 25 years.'), # Consistent\n",
    "    (gps_string, 'Type-1 equipment must be fail-safe, ensuring that any single-point failure (SPF) or multiple-point failure (MPF) does not cause harm to people, the equipment, or adjacent equipment.'), # Hallucinated\n",
    "    (gps_string, \"Type-1 equipment is fail-safe only when operated under controlled maintenance conditions and does not need to mitigate multiple-point failures (MPFs).\"),\n",
    "]\n",
    "\n",
    "# Step 1: Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'vectara/hallucination_evaluation_model', trust_remote_code=True)\n",
    "\n",
    "# Step 2: Use the model to predict\n",
    "predictions = model.predict(pairs) # note the predict() method. Do not do model(pairs). \n",
    "print(predictions)\n",
    "# tensor([0.0111, 0.6474, 0.1290, 0.8969, 0.1846, 0.0050, 0.0543])\n",
    "\n",
    "# Iterate through results and print scores\n",
    "for idx in range(len(pairs)):\n",
    "    print(f\"Source: {pairs[idx][0]}\")\n",
    "    print(f\"Response: {pairs[idx][1]}\")\n",
    "    print(f\"SCORE: {np.round(predictions[idx].item(), 3)}\\n\")  # Convert tensor to Python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
